{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "ws-azsynapse1984"
		},
		"AzureSynapseSQLPool_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSynapseSQLPool'"
		},
		"CosmosDbNoSql1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'CosmosDbNoSql1'"
		},
		"ls_adls_to_synapse_src_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'ls_adls_to_synapse_src'"
		},
		"ws-azsynapse1984-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ws-azsynapse1984-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:ws-azsynapse1984.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"ls_adls_to_synapse_src_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://authenticomvindetailsdl.dfs.core.windows.net/"
		},
		"ws-azsynapse1984-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://myadlsaccount1984sa.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/pl_copy_adlsgen2_to_synapse')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "GetZippedFileMetadata",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "ds_adlsgen2_to_synapse_src",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"exists",
								"itemName",
								"itemType",
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "DelimitedTextReadSettings"
							}
						}
					},
					{
						"name": "ForEachZippedFileName",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "DeleteCSVFilesFirst",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('GetZippedFileMetadata').output.childItems",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "CopyZippedFilesToStaging",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "DelimitedTextSource",
											"storeSettings": {
												"type": "AzureBlobFSReadSettings",
												"recursive": true,
												"wildcardFileName": {
													"value": "@item().name",
													"type": "Expression"
												},
												"enablePartitionDiscovery": false
											},
											"formatSettings": {
												"type": "DelimitedTextReadSettings",
												"compressionProperties": {
													"type": "ZipDeflateReadSettings",
													"preserveZipFileNameAsFolder": false
												}
											}
										},
										"sink": {
											"type": "DelimitedTextSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings",
												"copyBehavior": "FlattenHierarchy"
											},
											"formatSettings": {
												"type": "DelimitedTextWriteSettings",
												"quoteAllText": true,
												"fileExtension": ".txt"
											}
										},
										"enableStaging": false,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "ds_adlsgen2_to_synapse_src",
											"type": "DatasetReference",
											"parameters": {}
										}
									],
									"outputs": [
										{
											"referenceName": "ds_staging_path",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								}
							]
						}
					},
					{
						"name": "DeleteCSVFilesFirst",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "GetZippedFileMetadata",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "ds_staging_path",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"wildcardFileName": "*.csv",
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "CopyUnzippedFilesToSynapseSQL",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "ForEachZippedFileName",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "*.csv",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlDWSink",
								"preCopyScript": "truncate table dbo.UserData1",
								"writeBehavior": "Insert",
								"sqlWriterUseTableLock": false,
								"disableMetricsCollection": false
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "UserID",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "UserID",
											"type": "Int32",
											"physicalType": "int"
										}
									},
									{
										"source": {
											"name": "Name",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "Name",
											"type": "String",
											"physicalType": "varchar"
										}
									},
									{
										"source": {
											"name": "EmailID",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "EmailID",
											"type": "String",
											"physicalType": "varchar"
										}
									},
									{
										"source": {
											"name": "State",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "State",
											"type": "String",
											"physicalType": "varchar"
										}
									},
									{
										"source": {
											"name": "City",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "City",
											"type": "String",
											"physicalType": "varchar"
										}
									}
								],
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "ds_staging_path",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "ds_synapse_sql_pool_tgt",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/ds_adlsgen2_to_synapse_src')]",
				"[concat(variables('workspaceId'), '/datasets/ds_staging_path')]",
				"[concat(variables('workspaceId'), '/datasets/ds_synapse_sql_pool_tgt')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pl_running_a_notebook')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "SampleSparkNotebook",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Getting Started with Delta Lake",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkpoolprac84",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 8
							},
							"driverSize": "Small",
							"numExecutors": 2
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Getting Started with Delta Lake')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkpoolprac84')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ds_adlsgen2_to_synapse_src')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_to_synapse_src",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": " demozipfiles-ch04",
						"fileSystem": "synapse-exercises"
					},
					"columnDelimiter": ",",
					"compressionCodec": "ZipDeflate",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_to_synapse_src')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ds_staging_path')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_to_synapse_src",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "demozipfilestaging",
						"fileSystem": "staging"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "UserID",
						"type": "String"
					},
					{
						"name": "Name",
						"type": "String"
					},
					{
						"name": "EmailID",
						"type": "String"
					},
					{
						"name": "State",
						"type": "String"
					},
					{
						"name": "City",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_to_synapse_src')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ds_synapse_sql_pool_tgt')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSynapseSQLPool",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [
					{
						"name": "UserID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "Name",
						"type": "varchar"
					},
					{
						"name": "EmailID",
						"type": "varchar"
					},
					{
						"name": "State",
						"type": "varchar"
					},
					{
						"name": "City",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "UserData1"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSynapseSQLPool')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseSQLPool')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('AzureSynapseSQLPool_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CosmosDbNoSql1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('CosmosDbNoSql1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_adls_to_synapse_src')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "link service for providing connection information of ADLS gen 2 source data",
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ls_adls_to_synapse_src_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('ls_adls_to_synapse_src_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws-azsynapse1984-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('ws-azsynapse1984-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws-azsynapse1984-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ws-azsynapse1984-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tr_pl_adlsgen2_to_synapse')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "trgigger for running pipeline copying zipped file from adls gen 2 to azure synapse sql pool table UserData1",
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "pl_copy_adlsgen2_to_synapse",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2024-04-04T06:37:00",
						"timeZone": "India Standard Time"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/pl_copy_adlsgen2_to_synapse')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bringing Data to Synapse')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\nThere are several options to bring in data into Synapse. These are:\n1. Using Synapse pipelines to import data\n2. Using Azure Data Factory to import data\n3. Using SQL Server Integration Services (SSIS) to import data\n4. Using a COPY statement to import data\n*/\n\n-- Create DimEmployee Table\nCREATE TABLE [dbo].[DimEmployee]\n(\n    [EmployeeKey] [int]  NOT NULL,\n    [ParentEmployeeKey] [int] NULL,\n    [EmployeeNationalIDAlternateKey] [nvarchar](15) NULL,\n    [ParentEmployeeNationalIDAlternateKey] [nvarchar](15) NULL,\n    [SalesTerritoryKey] [int] NULL,\n    [FirstName] [nvarchar](50) NOT NULL,\n    [LastName] [nvarchar](50) NOT NULL,\n    [MiddleName] [nvarchar](50) NULL,\n    [NameStyle] [bit] NOT NULL,\n    [Title] [nvarchar](50) NULL,\n    [HireDate] [date] NULL,\n    [BirthDate] [date] NULL,\n    [LoginID] [nvarchar](256) NULL,\n    [EmailAddress] [nvarchar](50) NULL,\n    [Phone] [nvarchar](25) NULL,\n    [MaritalStatus] [nchar](1) NULL,\n    [EmergencyContactName] [nvarchar](50) NULL,\n    [EmergencyContactPhone] [nvarchar](25) NULL,\n    [SalariedFlag] [bit] NULL,\n    [Gender] [nchar](1) NULL,\n    [PayFrequency] [tinyint] NULL,\n    [BaseRate] [money] NULL,\n    [VacationHours] [smallint] NULL,\n    [SickLeaveHours] [smallint] NULL,\n    [CurrentFlag] [bit] NOT NULL,\n    [SalesPersonFlag] [bit] NOT NULL,\n    [DepartmentName] [nvarchar](50) NULL,\n    [StartDate] [date] NULL,\n    [EndDate] [date] NULL,\n    [Status] [nvarchar](50) NULL\n)\nGO\n\n-- Create UserData Table\nCREATE TABLE UserData (\n  registration_dttm datetime,\n  id int,\n  first_name varchar(200),\n  last_name varchar(200),\n  email varchar(200),\n  gender varchar(20),\n  ip_address varchar(200),\n  cc varchar(200),\n  country varchar(200),\n  birthdate varchar(200),\n  salary decimal,\n  title varchar(200),\n  comments varchar(1000)\n)\nGO\n\n-- Using Synapse Pipelines to import Data\nCREATE TABLE dbo.authenticom_sales (\n  File_Type VARCHAR(20)\n, DV_Dealer_ID NVARCHAR(50) NOT NULL\n, Vendor_Dealer_ID NVARCHAR(50) NULL\n, DMS_Type VARCHAR(50) NULL\n, Deal_Number INT NULL\n, Customer_Number NVARCHAR(50) NULL\n, Full_Name NVARCHAR(200) NULL\n, Salutation VARCHAR(20) NULL\n, First_Name NVARCHAR(100) NULL\n, Middle_Name NVARCHAR(100) NULL\n, Last_Name NVARCHAR(100) NULL\n, Suffix NVARCHAR(50) NULL\n, Address_Line_1 NVARCHAR(500) NULL\n, Address_Line_2 NVARCHAR(500) NULL\n, City NVARCHAR(100) NULL\n, State NVARCHAR(20) NULL\n, Zip NVARCHAR(50) NULL\n, County NVARCHAR(100) NULL\n, Home_Phone NVARCHAR(50) NULL\n, Cell_Phone NVARCHAR(50) NULL\n, Work_Phone NVARCHAR(50) NULL\n, Work_Extension INT NULL\n, Email_1 NVARCHAR(150) NULL\n, Email_2 NVARCHAR(150) NULL\n, Email_3 NVARCHAR(150) NULL\n, Birth_Date DATETIME NULL\n, Individual_Business_Flag NVARCHAR(20) NULL\n, Opt_Out NVARCHAR(50) NULL\n, Block_Email NVARCHAR(150) NULL\n, Block_Phone NVARCHAR(50) NULL\n, Block_Mail NVARCHAR(150) NULL\n, Language NVARCHAR(100) NULL\n, Customer_Create_Date DATETIME NULL\n, Customer_Last_Activity_Date DATETIME NULL\n, [Co-Buyer_Customer_Number] NVARCHAR(50) NULL\n, [Co-Buyer_Full_Name] NVARCHAR(200) NULL\n, [Co-Buyer_Salutation] NVARCHAR(20) NULL\n, [Co-Buyer_First_Name] NVARCHAR(100) NULL\n, [Co-Buyer_Middle_Name] NVARCHAR(100) NULL\n, [Co-Buyer_Last_Name] NVARCHAR(100) NULL\n, [Co-Buyer_Suffix] NVARCHAR(50) NULL\n, [Co-Buyer_Address_Line_1] NVARCHAR(500) NULL\n, [Co-Buyer_Address_Line_2] NVARCHAR(500) NULL\n, [Co-Buyer_City] NVARCHAR(100) NULL\n, [Co-Buyer_State] NVARCHAR(20) NULL\n, [Co-Buyer_Zip] NVARCHAR(50) NULL\n, [Co-Buyer_County] NVARCHAR(100) NULL\n, [Co-Buyer_Home_Phone] NVARCHAR(50) NULL\n, [Co-Buyer_Cell_Phone] NVARCHAR(50) NULL\n, [Co-Buyer_Work_Phone] NVARCHAR(50) NULL\n, [Co-Buyer_Work_Extension] NVARCHAR(20) NULL\n, [Co-Buyer_Email_1]  NVARCHAR(150) NULL\n, [Co-Buyer_Email_2]  NVARCHAR(150) NULL\n, [Co-Buyer_Email_3]  NVARCHAR(150) NULL\n, [Co-Buyer_Birth_Date] DATETIME NULL\n, [Co-Buyer_Individual_Business_Flag] NVARCHAR(20) NULL\n, [Co-Buyer_Opt_Out] NVARCHAR(50) NULL\n, VIN NVARCHAR(100) NULL\n, Year INT NULL\n, Make NVARCHAR(150) NULL\n, Model NVARCHAR(150) NULL\n, Model_Number NVARCHAR(100) NULL\n, Mileage INT NULL\n, Description NVARCHAR(1024) NULL\n, Exterior_Color NVARCHAR(150) NULL\n, New_Used NVARCHAR(50) NULL\n, Stock_Number NVARCHAR(50) NULL\n, Transmission NVARCHAR(50) NULL\n, Engine_Configuration NVARCHAR(250) NULL\n, Trim NVARCHAR(250) NULL\n, Engine_Number NVARCHAR(250) NULL\n, Chassis_Number NVARCHAR(250) NULL\n, License_Plate_Number NVARCHAR(100) NULL\n, Delivery_Date DATETIME NULL\n, Delivery_Mileage INT NULL\n, Inventory_Date DATETIME NULL\n, In_Service_Date DATETIME NULL\n, VIN_Explosion_Year INT NULL\n, VIN_Explosion_Make NVARCHAR(150) NULL\n, VIN_Explosion_Model NVARCHAR(150) NULL\n, VIN_Explosion_Trim NVARCHAR(150) NULL\n, VIN_Explosion_Transmission_Type NVARCHAR(150) NULL\n, VIN_Explosion_Fuel_Type NVARCHAR(100) NULL\n, VIN_Explosion_Engine_Size NVARCHAR(100) NULL\n, VIN_Explosion_GVW_Range NVARCHAR(150) NULL\n, Trade_1_VIN NVARCHAR(150) NULL\n, Trade_1_Year INT NULL\n, Trade_1_Make NVARCHAR(150) NULL\n, Trade_1_Model NVARCHAR(150) NULL\n, Trade_1_Odometer INT NULL\n, Trade_1_Actual_Cash_Value FLOAT NULL\n, Trade_1_Gross FLOAT NULL\n, Trade_1_Payoff FLOAT NULL\n, Trade_2_VIN NVARCHAR(150) NULL\n, Trade_2_Year INT NULL\n, Trade_2_Make NVARCHAR(150) NULL\n, Trade_2_Model NVARCHAR(150) NULL\n, Trade_2_Odometer INT NULL\n, Trade_2_Actual_Cash_Value FLOAT NULL\n, Trade_2_Gross FLOAT NULL\n, Trade_2_Payoff FLOAT NULL\n, Salesman_1_Number NVARCHAR(50) NULL\n, Salesman_1_Name NVARCHAR(250) NULL\n, Salesman_2_Number NVARCHAR(50) NULL\n, Salesman_2_Name NVARCHAR(250) NULL\n, Salesman_3_Number NVARCHAR(50) NULL\n, Salesman_3_Name NVARCHAR(250) NULL\n, Closing_Manager_Number NVARCHAR(50) NULL\n, Closing_Manager_Name NVARCHAR(250) NULL\n, Finance_Manager_Number NVARCHAR(50) NULL\n, Finance_Manager_Name NVARCHAR(250) NULL\n, Salesman_Manager_Number NVARCHAR(50) NULL\n, Salesman_Manager_Name NVARCHAR(250) NULL\n, MSRP FLOAT NULL\n, List_Price FLOAT NULL\n, Sales_Price FLOAT NULL\n, Journal_Price FLOAT NULL\n, Cost FLOAT NULL\n, Journal_Cost FLOAT NULL\n, Adjustments FLOAT NULL\n, Adjusted_Cost FLOAT NULL\n, Incentives FLOAT NULL\n, Pack_Amount FLOAT NULL\n, Sale_Net FLOAT NULL\n, Total_Trade_Actual_Cash_Value FLOAT NULL\n, Total_Trade_Gross FLOAT NULL\n, We_Owe_Front FLOAT NULL\n, Total_Front_Fees_Aftermarket_Profit FLOAT NULL\n, Total_Front_Commission FLOAT NULL\n, Total_Front_Sales FLOAT NULL\n, Total_Front_Cost FLOAT NULL\n, Front_Gross FLOAT NULL\n, Finance_Profit FLOAT NULL\n, Total_Warranty_Profit FLOAT NULL\n, We_Owe_Back FLOAT NULL\n, Insurance_Profit FLOAT NULL\n, Total_Back_Fees_Aftermarket_Profit FLOAT NULL\n, Finance_Reserve FLOAT NULL\n, Total_Back_Commission FLOAT NULL\n, Total_Back_Sales FLOAT NULL\n, Total_Back_Cost FLOAT NULL\n, Back_Gross FLOAT NULL\n, Total_Profit FLOAT NULL\n, Gross_Profit FLOAT NULL\n, Gross_Payable FLOAT NULL\n, Deal_Status NVARCHAR(50) NULL\n, Entry_Date DATETIME NULL\n, Booked_Date DATETIME NULL\n, Finalized_Date DATETIME NULL\n, Contract_Date DATETIME NULL\n, Accounting_Date DATETIME NULL\n, Status_Change_Date DATETIME NULL\n, First_Pay_Date DATETIME NULL\n, Deal_Type NVARCHAR(100) NULL\n, Sale_Type NVARCHAR(100) NULL\n, Bank_ID NVARCHAR(100) NULL\n, Bank_Name NVARCHAR(250) NULL\n, Bank_Address NVARCHAR(500) NULL\n, Term INT NULL\n, Amount_Financed FLOAT NULL\n, APR FLOAT NULL\n, Monthly_Payment FLOAT NULL\n, Payment_Total FLOAT NULL\n, Rebates FLOAT NULL\n, Deposit FLOAT NULL\n, Down_Payment FLOAT NULL\n, Total_Net_Trades FLOAT NULL\n, Total_Down FLOAT NULL\n, Balloon_Amount FLOAT NULL\n, Adjusted_Balloon_Amount FLOAT NULL\n, Holdback_Amount FLOAT NULL\n, Total_Drive_Off_Amount FLOAT NULL\n, License_Fee FLOAT NULL\n, Registration_Fee FLOAT NULL\n, Documentation_Fee FLOAT NULL\n, Finance_Charge FLOAT NULL\n, Total_Pickup_Payments FLOAT NULL\n, Sell_Rate FLOAT NULL\n, Buy_Rate FLOAT NULL\n, Residual_Rate FLOAT NULL\n, Residual_Amount FLOAT NULL\n, Allowed_Miles INT NULL\n, Estimated_Miles INT NULL\n, Mileage_Rate FLOAT NULL\n, Acquisition_Fee FLOAT NULL\n, Base_Payment FLOAT NULL\n, Security_Deposit FLOAT NULL\n, Total_Capital_Reduction FLOAT NULL\n, Net_Capital_Cost FLOAT NULL\n, Lease_Depreciation_Value FLOAT NULL\n, Dealer_Fees FLOAT NULL\n, Government_Fees FLOAT NULL\n, Total_Tax FLOAT NULL\n, Registration_State NVARCHAR(20) NULL\n, Report_of_Sale_Number NVARCHAR(50) NULL\n, Salesman_1_Total_Commission FLOAT NULL\n, Salesman_1_Front_Commission FLOAT NULL\n, Salesman_1_Back_Commission FLOAT NULL\n, Salesman_2_Total_Commission FLOAT NULL\n, Salesman_2_Front_Commission FLOAT NULL\n, Salesman_2_Back_Commission FLOAT NULL\n, Salesman_3_Total_Commission FLOAT NULL\n, Salesman_3_Front_Commission FLOAT NULL\n, Salesman_3_Back_Commission FLOAT NULL\n, Warranty_1_Name NVARCHAR(100) NULL\n, Warranty_1_Sale FLOAT NULL\n, Warranty_1_Cost FLOAT NULL\n, Warranty_1_Miles INT NULL\n, Warranty_1_Term INT NULL\n, Warranty_2_Name NVARCHAR(100) NULL\n, Warranty_2_Sale FLOAT NULL\n, Warranty_2_Cost FLOAT NULL\n, Warranty_2_Miles INT NULL\n, Warranty_2_Term INT NULL\n, Warranty_3_Name NVARCHAR(100) NULL\n, Warranty_3_Sale FLOAT NULL\n, Warranty_3_Cost FLOAT NULL\n, Warranty_3_Miles INT NULL\n, Warranty_3_Term INT NULL\n, Warranty_4_Name NVARCHAR(100) NULL\n, Warranty_4_Sale FLOAT NULL\n, Warranty_4_Cost FLOAT NULL\n, Warranty_4_Miles INT NULL\n, Warranty_4_Term INT NULL\n, Warranty_5_Name NVARCHAR(100) NULL\n, Warranty_5_Sale FLOAT NULL\n, Warranty_5_Cost FLOAT NULL\n, Warranty_5_Miles INT NULL\n, Warranty_5_Term INT NULL\n, Total_Fee_Aftermarket_Sale FLOAT NULL\n, Total_Fee_Aftermarket_Cost FLOAT NULL\n, Fee_Aftermarket_1_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_1_Sale FLOAT NULL\n, Fee_Aftermarket_1_Cost FLOAT NULL\n, Fee_Aftermarket_1_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_2_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_2_Sale FLOAT NULL\n, Fee_Aftermarket_2_Cost FLOAT NULL\n, Fee_Aftermarket_2_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_3_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_3_Sale FLOAT NULL\n, Fee_Aftermarket_3_Cost FLOAT NULL\n, Fee_Aftermarket_3_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_4_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_4_Sale FLOAT NULL\n, Fee_Aftermarket_4_Cost FLOAT NULL\n, Fee_Aftermarket_4_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_5_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_5_Sale FLOAT NULL\n, Fee_Aftermarket_5_Cost FLOAT NULL\n, Fee_Aftermarket_5_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_6_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_6_Sale FLOAT NULL\n, Fee_Aftermarket_6_Cost FLOAT NULL\n, Fee_Aftermarket_6_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_7_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_7_Sale FLOAT NULL\n, Fee_Aftermarket_7_Cost FLOAT NULL\n, Fee_Aftermarket_7_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_8_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_8_Sale FLOAT NULL\n, Fee_Aftermarket_8_Cost FLOAT NULL\n, Fee_Aftermarket_8_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_9_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_9_Sale FLOAT NULL\n, Fee_Aftermarket_9_Cost FLOAT NULL\n, Fee_Aftermarket_9_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_10_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_10_Sale FLOAT NULL\n, Fee_Aftermarket_10_Cost FLOAT NULL\n, Fee_Aftermarket_10_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_11_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_11_Sale FLOAT NULL\n, Fee_Aftermarket_11_Cost FLOAT NULL\n, Fee_Aftermarket_11_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_12_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_12_Sale FLOAT NULL\n, Fee_Aftermarket_12_Cost FLOAT NULL\n, Fee_Aftermarket_12_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_13_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_13_Sale FLOAT NULL\n, Fee_Aftermarket_13_Cost FLOAT NULL\n, Fee_Aftermarket_13_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_14_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_14_Sale FLOAT NULL\n, Fee_Aftermarket_14_Cost FLOAT NULL\n, Fee_Aftermarket_14_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_15_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_15_Sale FLOAT NULL\n, Fee_Aftermarket_15_Cost FLOAT NULL\n, Fee_Aftermarket_15_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_16_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_16_Sale FLOAT NULL\n, Fee_Aftermarket_16_Cost FLOAT NULL\n, Fee_Aftermarket_16_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_17_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_17_Sale FLOAT NULL\n, Fee_Aftermarket_17_Cost FLOAT NULL\n, Fee_Aftermarket_17_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_18_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_18_Sale FLOAT NULL\n, Fee_Aftermarket_18_Cost FLOAT NULL\n, Fee_Aftermarket_18_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_19_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_19_Sale FLOAT NULL\n, Fee_Aftermarket_19_Cost FLOAT NULL\n, Fee_Aftermarket_19_Profit_Indicator NVARCHAR(200) NULL\n, Fee_Aftermarket_20_Name NVARCHAR(200) NULL\n, Fee_Aftermarket_20_Sale FLOAT NULL\n, Fee_Aftermarket_20_Cost FLOAT NULL\n, Fee_Aftermarket_20_Profit_Indicator NVARCHAR(200) NULL\n, Insurance_1_Type NVARCHAR(100) NULL\n, Insurance_1_Name NVARCHAR(200) NULL\n, Insurance_1_Sale FLOAT NULL\n, Insurance_1_Cost FLOAT NULL\n, Insurance_1_Term INT NULL\n, Insurance_2_Type NVARCHAR(100) NULL\n, Insurance_2_Name NVARCHAR(200) NULL\n, Insurance_2_Sale FLOAT NULL\n, Insurance_2_Cost FLOAT NULL\n, Insurance_2_Term INT NULL\n, Insurance_3_Type NVARCHAR(100) NULL\n, Insurance_3_Name NVARCHAR(200) NULL\n, Insurance_3_Sale FLOAT NULL\n, Insurance_3_Cost FLOAT NULL\n, Insurance_3_Term INT NULL\n, Accidental_Health_Type NVARCHAR(100) NULL\n, Accidental_Health_Name NVARCHAR(100) NULL\n, Accidental_Health_Sale FLOAT NULL\n, Accidental_Health_Cost FLOAT NULL\n, Accidental_Health_Term INT NULL\n, Credit_Life_Type NVARCHAR(100) NULL\n, Credit_Life_Name NVARCHAR(100) NULL\n, Credit_Life_Sale FLOAT NULL\n, Credit_Life_Cost FLOAT NULL\n, Credit_Life_Term INT NULL\n, Levelized_Life_Type NVARCHAR(100) NULL\n, Levelized_Life_Name NVARCHAR(100) NULL\n, Levelized_Life_Sale FLOAT NULL\n, Levelized_Life_Cost FLOAT NULL\n, Levelized_Life_Term INT NULL\n, Loss_of_Employment_Type NVARCHAR(100) NULL\n, Loss_of_Employment_Name NVARCHAR(100) NULL\n, Loss_of_Employment_Sale FLOAT NULL\n, Loss_of_Employment_Cost FLOAT NULL\n, Loss_of_Employment_Term INT NULL\n, Guaranteed_Auto_Protection_Type NVARCHAR(100) NULL\n, Guaranteed_Auto_Protection_Name NVARCHAR(100) NULL\n, Guaranteed_Auto_Protection_Sale FLOAT NULL\n, Guaranteed_Auto_Protection_Cost FLOAT NULL\n, Guaranteed_Auto_Protection_Term INT NULL\n, Sale_Comments NVARCHAR(1024) NULL\n, CASS_STD_LINE1 NVARCHAR(250) NULL\n, CASS_STD_LINE2 NVARCHAR(250) NULL\n, CASS_STD_CITY NVARCHAR(100) NULL\n, CASS_STD_STATE NVARCHAR(20) NULL\n, CASS_STD_ZIP NVARCHAR(50) NULL\n, CASS_STD_ZIP4 INT NULL\n, CASS_STD_DPBC INT NULL\n, CASS_STD_CHKDGT INT NULL\n, CASS_STD_CART NVARCHAR(100) NULL\n, CASS_STD_LOT INT NULL\n, CASS_STD_LOTORD NVARCHAR(100) NULL\n, CASS_STD_URB NVARCHAR(100) NULL\n, CASS_STD_FIPS NVARCHAR(100) NULL\n, CASS_STD_EWS NVARCHAR(100) NULL\n, CASS_STD_LACS NVARCHAR(100) NULL\n, CASS_STD_ZIPMOV INT NULL\n, CASS_STD_Z4LOM NVARCHAR(100) NULL\n, CASS_STD_NDIAPT INT NULL\n, CASS_STD_NDIRR NVARCHAR(100) NULL\n, CASS_STD_LACSRT INT NULL\n, CASS_STD_ERROR_CD NVARCHAR(100) NULL\n, Co_Buyer_Block_Email NVARCHAR(250) NULL\n, Co_Buyer_Block_Phone NVARCHAR(100) NULL\n, Co_Buyer_Block_Mail NVARCHAR(100) NULL\n, CreatedDate DATETIME NOT NULL\n, RunSeqNO INT NOT NULL\n, ingestion_dt DATETIME NOT NULL\n);\n\n--test data copied into authenticom_sales table\nselect CreatedDate, count(*) from dbo.authenticom_sales\ngroup by CreatedDate\norder by CreatedDate;\n\n-- Using Azure Data Factory to import data\nCREATE TABLE dbo.authenticom_service (\n  identifier NVARCHAR(100)\n, Payment_Method NVARCHAR(50) NOT NULL\n, Operation_Codes NVARCHAR(50) NULL\n, Tech_Number NVARCHAR(50) NULL\n, Tech_Name NVARCHAR(250) NULL\n, Upsell NVARCHAR(50) NULL\n, Recommended_Operation_Codes NVARCHAR(200) NULL\n, Recommendations NVARCHAR(1024) NULL\n, Operation_Line_Number NVARCHAR(100) NULL\n, Operation_Sale_Types NVARCHAR(100) NULL\n, Operation_Line_Cost NVARCHAR(100) NULL\n, Operation_Line_Sale NVARCHAR(50) NULL\n, Misc_Cost NVARCHAR(50) NULL\n, Misc_Sale NVARCHAR(50) NULL\n, Gas_Oil_Grease_Cost NVARCHAR(100) NULL\n, Gas_Oil_Grease_Sale NVARCHAR(100) NULL\n, Sublet_Cost NVARCHAR(50) NULL\n, Sublet_Sale NVARCHAR(100) NULL\n, Tech_Labor_Line NVARCHAR(100) NULL\n, Labor_Cause NVARCHAR(1024) NULL\n, Labor_Complaint NVARCHAR(1024) NULL\n, Labor_Correction NVARCHAR(1024) NULL\n, Labor_Cost NVARCHAR(100) NULL\n, Labor_Sale NVARCHAR(100) NULL\n, Labor_Tech_Hours NVARCHAR(100) NULL\n, Labor_Bill_Hours NVARCHAR(100) NULL\n, Labor_Tech_Rate NVARCHAR(100) NULL\n, Labor_Bill_Rate NVARCHAR(100) NULL\n, Parts_Labor_Line_Number NVARCHAR(100) NULL\n, Parts_Cost NVARCHAR(100) NULL\n, Parts_Sale NVARCHAR(100) NULL\n, Parts_Line_Number NVARCHAR(100) NULL\n, Parts_Sale_Type NVARCHAR(100) NULL\n, Part_Number NVARCHAR(250) NULL\n, Part_Description NVARCHAR(250) NULL\n, Part_Quantity NVARCHAR(100) NULL\n, Parts_Unit_Cost NVARCHAR(100) NULL\n, Parts_Unit_Sale NVARCHAR(100) NULL\n, VIN_MASK NVARCHAR(250) NULL\n, CreatedDate DATETIME NULL\n, RunSeqNo INT NOT NULL\n, ingestion_dt DATETIME NOT NULL);\n\n-- validate data copy using azure data factory\nselect CreatedDate, count(*) from dbo.authenticom_service\ngroup by CreatedDate\norder by CreatedDate;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "synapsesqlpool84",
						"poolName": "synapsesqlpool84"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL pool validation queries')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- get the count of control nodes and compute nodes in synapse dedicated SQL pool\nselect * from sys.dm_pdw_nodes\nGO\nselect type, count(1)\nfrom sys.dm_pdw_nodes\ngroup by type;\n\n--view the current DWU setting\nselect db.name [Database]\n, ds.edition [Edition]\n, ds.service_objective [Service Objective]\nfrom sys.database_service_objectives AS ds\nJOIN sys.databases AS db ON ds.database_id = db.database_id;\n\n-- Dynamic Management Views (DMVs) to know more about your nodes and distributions\nselect distribution_id, pdw_node_id from sys.pdw_nodes_partitions\nGO\nselect distribution_id, pdw_nodel_id from sys.pdw_distributions\n\n-- Create a HASH distributed table\nCREATE TABLE FactSales (\n  SalesID INT IDENTITY(1, 1) NOT NULL, \n  SalesDate DATETIME NOT NULL, \n  SalesItemId INT,\n  Description VARCHAR(500)\n) WITH ( CLUSTERED INDEX (SalesID), DISTRIBUTION = \nHASH(SalesDate) );\n\n/*\ninsert into dbo.FactSales (SalesDate, SalesItemId, Description)\nvalues ('2024-03-11 02:10:18.145', 15, 'bought a notepad');\n*/\n\n/* 1. uses a hash function to distribute table rows across the available compute nodes\n2. this type of distribution is created to minimize data movement across various distributions because similar values tend to fall within the same distribution\n3. Tables that are more than 2 Gigabytes (GB) in size on disk and tables having more frequent \nINSERT, UPDATE, and DELETE operations are the best candidates for hash distributions\n*/\n\n-- Create a round robin (default distriubtion type in SQL pool)\nCREATE TABLE DimSalesItem (\n  SalesItemID INT IDENTITY(1, 1) NOT NULL, \n  Description VARCHAR(500)\n) WITH ( CLUSTERED INDEX (SalesItemID), DISTRIBUTION = ROUND_ROBIN );\n\n/*\n1. default distribution type for a table in SQL Pool is round-robin distribution, whereby \ndata is divided evenly across all the distributions\n2. Sometimes, rows need to be reshuffled when you perform joining operations on round-robin distributed tables\n3. Temporary staging tables and tables with no obvious joining key are the best candidates \nfor ROUND_ROBIN distributed tables\n*/\n\n-- Create Replicated tables\nCREATE TABLE [dbo].[DimSalesRegion] \n(\n    RegionID INT IDENTITY(1,1) NOT NULL,\n    Region VARCHAR(50) NOT NULL\n)\n    WITH ( CLUSTERED COLUMNSTORE INDEX, DISTRIBUTION = \nREPLICATE )\n\n/*\n1. Replicated tables must only be used for small dimension tables\n2. This replicates the table data across all distributions so that the data becomes local to each compute node and \naccessibility becomes easier\n3. There is no need to move the data across various compute nodes for a running query, and this \nhelps to return results very quickly\n4. Small dimension tables with a size less than 2 GB are the best candidates for replicated tables\n*/\n\n-- to check data skew for a distributed table\nDBCC PDW_SHOWSPACEUSED('dbo.FactSales');\n\n-- an example of using partitions along with distributions\nCreate table [dbo].[FactSales_new] (\n    [SalesID] int NOT NULL,\n    [OrderDateKey] int NOT NULL,\n    [CustomerKey] int NOT NULL,\n    [PromotionKey] int NOT NULL,\n    [ProductKey] int NOT NULL,\n    [SalesOrderNumber] nvarchar(20) NOT NULL,\n    [OrderQuantity] smallint NOT NULL, \n    [UnitPrice] money NOT NULL,\n    [SalesAmount] money NOT NULL\n) with (\n    CLUSTERED COLUMNSTORE INDEX,\n    DISTRIBUTION = HASH([ProductKey]),\n    PARTITION(\n        [OrderDateKey] RANGE RIGHT FOR\n        VALUES\n        (\n            20000101, 20010101, 20020101, 20030101,\n            20040101, 20050101\n        )\n    )\n);\n\n-- get details about all the nodes, distributions, and partitions\nSELECT B.distribution_id, A.pdw_node_id, A.[type] AS node_type, A.name AS node_name,\nC.partition_number,C.[rows] FROM sys.dm_pdw_nodes A\nLEFT JOIN sys.pdw_distributions B ON A.pdw_node_id=B.pdw_node_id\nLEFT JOIN sys.pdw_nodes_partitions C ON B.distribution_id=C.\ndistribution_id AND A.pdw_node_id=C.pdw_node_id\nORDER BY B.distribution_id\nGO\n\n-- create temporary tables\nIF OBJECT_ID('tempdb..#tempFactSales') IS NOT NULL\nBEGIN\n    DROP TABLE #tempFactSales\nEND\nGO\nCREATE TABLE #tempFactSales\nWITH\n(\n    DISTRIBUTION = HASH([SalesID])\n,    HEAP\n)\nAS\n(\nSELECT SalesId,SalesItemId, Description\nFROM dbo.FactSales\n);\nSELECT * FROM #tempFactSales\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "synapsesqlpool84",
						"poolName": "synapsesqlpool84"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse pipelines to orchestrate data')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Creating user table\nCREATE TABLE UserData1 (\n  UserID INT,\n  Name VARCHAR(200),\n  EmailID VARCHAR(200),\n  State VARCHAR(50),\n  City VARCHAR(50)\n);\n\n-- validating presence of data after copying it from zipped files in ADLS to synapse SQL pool\nselect * from dbo.UserData1\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "synapsesqlpool84",
						"poolName": "synapsesqlpool84"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Delta Lake')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpoolprac84",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a69c6b73-7a84-4bb4-94d8-2bba042474d0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/acdf152f-3867-4db8-9da5-81dfde654c21/resourceGroups/rg-dev-revenue-practice/providers/Microsoft.Synapse/workspaces/ws-azsynapse1984/bigDataPools/sparkpoolprac84",
						"name": "sparkpoolprac84",
						"type": "Spark",
						"endpoint": "https://ws-azsynapse1984.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolprac84",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Python)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"delta_table_path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"\n",
							"delta_table_path"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(0,5)\n",
							"data.show()\n",
							"data.write.format(\"delta\").save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").load(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(5,10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable1\")"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"source": [
							"# Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(delta_table_path))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show()\n",
							"\n",
							"# Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable1\").show(truncate=False)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Update every even value by adding 100 to it\n",
							"delta_table.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = { \"id\": expr(\"id + 100\") })\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete every even value\n",
							"delta_table.delete(\"id % 2 == 0\")\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) new data\n",
							"new_data = spark.range(0,20).alias(\"newData\")\n",
							"\n",
							"delta_table.alias(\"oldData\")\\\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\")\\\n",
							"    .whenMatchedUpdate(set = { \"id\": lit(\"-1\")})\\\n",
							"    .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") })\\\n",
							"    .execute()\n",
							"\n",
							"delta_table.toDF().show(100)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().show(20, 1000, False)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"streaming_df = spark.readStream.format(\"rate\").load()\n",
							"stream = streaming_df\\\n",
							"    .selectExpr(\"value as id\")\\\n",
							"    .writeStream\\\n",
							"    .format(\"delta\")\\\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id))\\\n",
							"    .start(delta_table_path)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.toDF().sort(col(\"id\").desc()).show(100)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(100, 1000, False)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"partition_count = 2\n",
							"\n",
							"spark.read\\\n",
							"    .format(\"delta\")\\\n",
							"    .load(delta_table_path)\\\n",
							"    .repartition(partition_count)\\\n",
							"    .write.option(\"dataChange\", \"false\")\\\n",
							"    .format(\"delta\")\\\n",
							"    .mode(\"overwrite\")\\\n",
							"    .save(delta_table_path)    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = \"/parquet/parquet-table-{0}\".format(session_id)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, \"parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"# Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DESCRIBE HISTORY delta.`{0}`\".format(delta_table_path)).show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"VACUUM delta.`{0}`\".format(delta_table_path)).show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_id = random.randint(0,1000)\n",
							"parquet_path = \"/parquet/parquet-table-{0}-{1}\".format(session_id, parquet_path)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)\n",
							"\n",
							"# Use SQL to convert the parquet table to Delta\n",
							"spark.sql(\"CONVERT TO DELTA parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SynapseLinkCosmosDB')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpoolprac84",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "491c6b58-a893-4ac6-90b5-161c601c44c6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/acdf152f-3867-4db8-9da5-81dfde654c21/resourceGroups/rg-dev-revenue-practice/providers/Microsoft.Synapse/workspaces/ws-azsynapse1984/bigDataPools/sparkpoolprac84",
						"name": "sparkpoolprac84",
						"type": "Spark",
						"endpoint": "https://ws-azsynapse1984.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolprac84",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Loading Cosmos DB data to a Spark DataFrame without impacting the transactional store\r\n",
							"df = spark.read.format(\"cosmos.olap\")\\\r\n",
							"    .option(\"spark.synapse.linkedService\", \"CosmosDbNoSql1\")\\\r\n",
							"    .option(\"spark.cosmos.container\", \"Products\")\\\r\n",
							"    .load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"```\r\n",
							"# To select a preferred list of regions in a multi-region Azure Cosmos DB account, add \r\n",
							".option(\"spark.cosmos.preferredRegions\", \r\n",
							"\"<Region1>,<Region2>\")\r\n",
							"```"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#  Write a Spark DataFrame into an Azure Cosmos DB container\r\n",
							"DataFrameName.write.format(\"cosmos.oltp\")\\\r\n",
							"    .option(\"spark.synapse.linkedService\", \"CosmosDbNoSql1\")\\\r\n",
							"    .option(\"spark.cosmos.container\", \"Products\")\\\r\n",
							"    .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\r\n",
							"    .mode('append')\\\r\n",
							"    .save()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SynapseLinkDemoNotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpoolprac84",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1095a457-7b27-41b1-9a4e-2cee1f9df205"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/acdf152f-3867-4db8-9da5-81dfde654c21/resourceGroups/rg-dev-revenue-practice/providers/Microsoft.Synapse/workspaces/ws-azsynapse1984/bigDataPools/sparkpoolprac84",
						"name": "sparkpoolprac84",
						"type": "Spark",
						"endpoint": "https://ws-azsynapse1984.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolprac84",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Working with \"StoreDemoGraphics\" sample file"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Reading data from non primary container (bronze) in primary storage account\r\n",
							"dfStoreDemoGraphics = (spark.read.csv(\r\n",
							"    \"abfss://azure-synapse-prac1984@myadlsaccount1984sa.dfs.core.windows.net/demosynapselink-ch05/StoreDemoGraphics.csv\",\r\n",
							"    header=True, inferSchema=True\r\n",
							"))"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##### Reading data from storage account"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reading data from non primary container (bronze) in primary storage account\r\n",
							"dfStoreDemoGraphics = (spark.read.csv(\r\n",
							"    \"/demosynapselink-ch05/StoreDemoGraphics.csv\",\r\n",
							"    header=True, inferSchema=True\r\n",
							"))"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(dfStoreDemoGraphics)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##### Writing Spark Dataframe to Cosmos DB container"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Writing spark dataframe into cosmos DB container\r\n",
							"dfStoreDemoGraphics.write.format(\"cosmos.oltp\") \\\r\n",
							".option(\"spark.synapse.linkedService\", \"CosmosDbNoSql1\")\\\r\n",
							".option(\"spark.cosmos.container\", \"StoreDemoGraphics\")\\\r\n",
							".option(\"spark.cosmos.write.bulk.enabled\", \"true\").mode(\"append\").save()"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reading data back from cosmos DB (to validate)\r\n",
							"df = spark.read.format(\"cosmos.oltp\")\\\r\n",
							".option(\"spark.synapse.linkedService\", \"CosmosDbNoSql1\")\\\r\n",
							".option(\"spark.cosmos.container\", \"StoreDemoGraphics\").load()"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"References:\r\n",
							"https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/tutorial-spark-connector?tabs=python&pivots=programming-language-python#partial-document-update-using-patch\r\n",
							"\r\n",
							"https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/cosmos/azure-cosmos-spark_3_2-12/docs/configuration-reference.md"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"##### Creating a Spark Table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"create table sample_table using cosmos.olap options (\r\n",
							"    spark.synapse.linkedService 'CosmosDbNoSql1',\r\n",
							"    spark.cosmos.container 'StoreDemoGraphics',\r\n",
							"    spark.cosmos.autoSchemaMerge 'true'\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from sample_table"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Working with \"Products\" sample file"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reading data from storage account and primary container\r\n",
							"dfProducts = (spark.read.csv(\r\n",
							"    \"/demosynapselink-ch05/Products.csv\", header=True, inferSchema=True\r\n",
							"))"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(dfProducts)"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Writing data to cosmos DB container\r\n",
							"dfProducts.write.format(\"cosmos.oltp\")\\\r\n",
							".option(\"spark.synapse.linkedService\", \"CosmosDbNoSql1\")\\\r\n",
							".option(\"spark.cosmos.container\", \"Products\")\\\r\n",
							".option(\"spark.cosmos.write.bulk.enabled\", \"true\").mode(\"append\").save()"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkpoolprac84')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 30
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsesqlpool84')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}